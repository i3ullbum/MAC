wandb_log: true
wandb_entity: i3ullbum
wandb_project: llm_meta_cont
wandb_key: ae076492b5c8d04d5c85accebf040565f86dceab
CACHE_DIR: null
seed: 42
resume: false
debug: true
early_stop: false
no_date: false
rank: 0
suffix: null
resume_path: null
no_reset: false
optimizer: adam
base_model: gpt2-large
tokenizer_name: gpt2
base_model_state_dict_path: null
load_path: null
best_val_loss: null
best_em: null
best_f1: null
batchs_per_base_reset: 0
port: 9819
distributed: false
world_size: 1
log_path: null
multiproc: true
use_accelerator: true
no_eval: false
server_info: null
n_epochs: 50
val_steps: 250
save_steps: false
outer_lr: 1.0e-05
grad_clip_thresh: 100.0
warmup_ratio: 0.01
context_opt: false
lr_schedule: constant_with_warmup
mixed_precision: bf16
weight_decay: 0.0
gpt_drop: true
quant_type: null
quant_compute_dtype: null
load_from_hf: false
mode: amortize_encdec
pretrained_model_amort: t5-base
question_model_amort: null
tokenizer_name_amort: t5-base
token_dim: null
num_virtual_tokens: 24
inner_lr: 0.3
num_inner_steps: 1
lift_ratio: 0.25
qencoder_type: encdec
normalize: false
pretrain_summarization: false
pretrain_summarization_model: t5-base
num_cross_attention_blocks: 4
sample_weights: true
sample_steps: ${val_steps}
null_shift: false
nul_shift_lam: null
qencoder_init: false
hierarchy_aware: false
hierarchy_aware_p: 0.0
update_batch_size: 4
update_val_batch_size: 12
grad_acc_steps: 8
context_window_list:
- 16
- 32
bm_learned_layers: -1
layer_num_virtual_tokens: 2
dropout_p: 0.0
neftune: false
noise_alpha: 5.0
no_aggregate: true
log_stepwise_metrics: false
use_pretrained: false
encdec_lora: false
qenc_lora: false
lora_rank: 32
lora_alpha: 16
lora_dropout: 0.05
dataset: streamingqa
data_dir: ./data/StreamingQA/
train_path: ${data_dir}/train.csv
val_path: ${data_dir}/val.csv
test_path: ${data_dir}/test.csv
lt_train_path: ${data_dir}/qa_train.csv
lt_val_path: ${data_dir}/qa_val.csv
