# @package _global_

mode: 'amortize_encdec'
base_model: 'gpt2-large'

pretrained_model_amort: 't5-base'
question_model_amort: null
tokenizer_name_amort: 't5-base'

token_dim: null
num_virtual_tokens: 24

inner_lr: 0.3
num_inner_steps: 1
lift_ratio: 0.25

qencoder_type: 'encdec'
normalize: False

pretrain_summarization: False
pretrain_summarization_model: t5-base


num_cross_attention_blocks: 4
sample_weights: True
sample_steps: ${val_steps}
null_shift: False
nul_shift_lam: null


qencoder_init: False

hierarchy_aware: False
hierarchy_aware_p: 0.0

seed: 42
update_batch_size: 4
update_val_batch_size: 12
grad_acc_steps: 8
context_window_list: [16, 32]

bm_learned_layers: -1

layer_num_virtual_tokens: 2
dropout_p: 0.0
neftune: False
noise_alpha: 5.
no_aggregate: False

log_stepwise_metrics: False

use_pretrained: False
base_model_state_dict_path: null

# LoRA
encdec_lora: False
qenc_lora: False
lora_rank: 32
lora_alpha: 16
lora_dropout: 0.05

hydra:
  run:
    dir: .