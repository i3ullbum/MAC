wandb_log: true
wandb_entity: 'i3ullbum'
wandb_project: 'llm_meta_cont'
wandb_key: 'ae076492b5c8d04d5c85accebf040565f86dceab'
CACHE_DIR: null

defaults:
  - _self_
  - mode_eval: ???
  - dataset: streamingqa

seed: 42
resume: False
debug: True
early_stop: False
no_date: False
rank: 0
suffix: null
resume_path: null
eval_no_adapt: False

downsample_to: 1665

base_model: 'distilgpt2'
tokenizer_name: 'gpt2'
base_model_state_dict_path: null
log_dir: null
load_path: null
distributed: False

log_path: null
multiproc: True
use_accelerator: True  # not supported for CaMeLS
no_eval: False
server_info: null

n_epochs: 50
val_steps: 250  # every 250 * grad_acc_steps steps, i.e., 1000val_steps: 100
save_steps: False

outer_lr: 1e-5
grad_clip_thresh: 100.
warmup_ratio: 0.01
context_opt: False
lr_schedule: 'constant_with_warmup'  # 'cosine' 'constant_with_warmup' 'constant'
mixed_precision: null
weight_decay: 0.0
grad_checkpointing: False
gpt_drop: True
no_base_model: True
quant_type: null
quant_compute_dtype: null
load_from_hf: False
llama_cache_dir: null
measure_hierarchy_memory: False
