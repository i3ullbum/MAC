wandb_log: true
wandb_entity: 'i3ullbum'
wandb_project: 'llm_meta_cont'
wandb_key: 'ae076492b5c8d04d5c85accebf040565f86dceab'
CACHE_DIR: null

defaults:
  - _self_
  - mode: ???
  - dataset: streamingqa

seed: 42
resume: True
debug: True
early_stop: False
no_date: False
rank: 0
suffix: null
resume_path: './logs/streamingqa/gpt2-large/240317_amortize_encdec_bs32_adam_lrconstant_with_warmup_outlr1e-05_amortt5-base_tokens24_Ltok2_no_aggregate_gptdrop_mpbf16_seed_42/checkpoints/best_em.pt'
no_reset: False
optimizer: 'adam'

base_model: 'distilgpt2'
tokenizer_name: 'gpt2'
base_model_state_dict_path: null
load_path: null
best_val_loss: null
best_em: null
best_f1: null
batchs_per_base_reset: 0
port: 9819
distributed: False
world_size: 1

log_path: null
multiproc: True
use_accelerator: True
no_eval: False
server_info: null

n_epochs: 50
val_steps: 250  # every 250 * grad_acc_steps steps, i.e., 1000val_steps: 100
save_steps: False

outer_lr: 1e-5
grad_clip_thresh: 100.
warmup_ratio: 0.01
context_opt: False
lr_schedule: 'constant_with_warmup'  # 'cosine' 'constant_with_warmup' 'constant'
mixed_precision: null
weight_decay: 0.0
gpt_drop: True
quant_type: null
quant_compute_dtype: null
load_from_hf: False

agg_option: 'document-wise'
